
@book{abelson1995,
  title = {Statistics {{As Principled Argument}}},
  author = {Abelson, Robert P},
  year = {1995},
  publisher = {{Taylor \& Francis}},
  address = {{New York}},
  file = {/Users/shilaan/Zotero/storage/8AL22UU2/Abelson - Statistics As Principled Argument.pdf},
  language = {en}
}

@book{apa2020,
  title = {Publication {{Manual}} of the {{American Psychological Association}}},
  author = {APA},
  year = {2019},
  edition = {Seventh edition},
  publisher = {{American Psychological Association}},
  address = {{Washington}},
  abstract = {"The Publication Manual of the American Psychological Association, Seventh Edition is the official source for APA Style. With millions of copies sold worldwide in multiple languages, it is the style manual of choice for writers, researchers, editors, students, and educators in the social and behavioral sciences, natural sciences, nursing, communications, education, business, engineering, and other fields. Known for its authoritative, easy-to-use reference and citation system, the Publication Manual also offers guidance on choosing the headings, tables, figures, language, and tone that will result in powerful, concise, and elegant scholarly communication. It guides users through the scholarly writing process-from the ethics of authorship to reporting research through publication. The seventh edition is an indispensable resource for students and professionals to achieve excellence in writing and make an impact with their work. The seventh edition has been thoroughly revised and updated to reflect best practices in scholarly writing and publishing. All formats are in full color, with a new tabbed spiral version Improved ease of navigation, with many additional numbered sections to help users quickly locate answers to their questions Resources for students on writing and formatting annotated bibliographies, response papers, and other paper types as well as guidelines on citing course materials Dedicated chapter for new users of APA Style covering paper elements and format, including sample papers for both professional authors and student writers New chapter on journal article reporting standards that includes updates to reporting standards for quantitative research and the first-ever qualitative and mixed methods reporting standards in APA Style New chapter on bias-free language guidelines for writing about people with respect and inclusivity in areas including age, disability, gender, participation in research, race and ethnicity, sexual orientation, socioeconomic status, and intersectionality More than 100 new reference examples covering periodicals, books, audiovisual media, social media, webpages and websites, and legal resources More than 40 new sample tables and figures Expanded guidance on ethical writing and publishing practices, including how to ensure the appropriate level of citation, avoid plagiarism and self-plagiarism, and navigate the publication process Guidelines that support accessibility for all users, including simplified reference, in-text citation, and heading formats as well as additional font options"--},
  file = {/Users/shilaan/Zotero/storage/LWVWXMZ3/American Psychological Association (Washington, District of Columbia) - 2019 - Publication manual of the American psychological a.pdf},
  isbn = {978-1-4338-3215-4 978-1-4338-3216-1 978-1-4338-3217-8},
  language = {en}
}

@article{bakker2012,
  title = {The {{Rules}} of the {{Game Called Psychological Science}}},
  author = {Bakker, Marjan and {van Dijk}, Annette and Wicherts, Jelte M.},
  year = {2012},
  month = nov,
  volume = {7},
  pages = {543--554},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691612459060},
  abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96\% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {$<$} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
  file = {/Users/shilaan/Zotero/storage/LHTJ6EX5/Bakker et al. - 2012 - The Rules of the Game Called Psychological Science.pdf},
  journal = {Perspectives on Psychological Science},
  number = {6}
}

@techreport{beffarabret2018,
  title = {A Fully Automated, Transparent, Reproducible, and Blind Protocol for Sequential Analyses},
  author = {Beffara Bret, Brice and Beffara Bret, Am{\'e}lie and Nalborczyk, Ladislas},
  year = {2018},
  month = feb,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/v7xpg},
  abstract = {Despite many cultural, methodological and technical improvements, one of the major obstacle to results reproducibility remains the pervasive low statistical power. In response to this problem, a lot of attention has recently been drawn to sequential analyses. This type of procedure has been shown to be more efficient (to require less observations and therefore less resources) than classical fixed-N procedures. However, these procedures are submitted to both intrapersonal and interpersonal biases during data collection and data analysis. In this tutorial, we explain how automation can be used to prevent these biases. We show how to synchronise open and free experiment software programs with the Open Science Framework and how to automate sequential data analyses in R. This tutorial is intended to researchers with beginner experience with R but no previous experience with sequential analyses is required.},
  file = {/Users/shilaan/Zotero/storage/3BJNPBI3/Beffara Bret et al. - 2018 - A fully automated, transparent, reproducible, and .pdf},
  language = {en},
  type = {Preprint}
}

@article{blume2002,
  title = {Likelihood Methods for Measuring Statistical Evidence},
  author = {Blume, Jeffrey D.},
  year = {2002},
  month = sep,
  volume = {21},
  pages = {2563--2599},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.1216},
  abstract = {Focused on interpreting data as statistical evidence, the evidential paradigm uses likelihood ratios to measure the strength of statistical evidence. Under this paradigm, re-examination of accumulating evidence is encouraged because (i) the likelihood ratio, unlike a p-value, is una ected by the number of examinations and (ii) the probability of observing strong misleading evidence is naturally low, even for study designs that re-examine the data with each new observation. Further, the controllable probabilities of observing misleading and weak evidence provide assurance that the study design is reliable without a ecting the strength of statistical evidence in the data. This paper illustrates the ideas and methods associated with using likelihood ratios to measure statistical evidence. It contains a comprehensive introduction to the evidential paradigm, including an overview of how to quantify the probability of observing misleading evidence for various study designs. The University Group Diabetes Program (UGDP), a classic and still controversial multi-centred clinical trial, is used as an illustrative example. Some of the original UGDP results, and subsequent re-analyses, are presented for comparison purposes. Copyright ? 2002 John Wiley \& Sons, Ltd.},
  file = {/Users/shilaan/Zotero/storage/S38HJBZG/Blume - 2002 - Likelihood methods for measuring statistical evide.pdf},
  journal = {Statistics in Medicine},
  language = {en},
  number = {17}
}

@article{button2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  volume = {14},
  pages = {365--376},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
  file = {/Users/shilaan/Zotero/storage/PPE8VAE3/Button et al. - 2013 - Power failure why small sample size undermines th.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {5}
}

@article{dienes2011,
  title = {Bayesian {{Versus Orthodox Statistics}}: {{Which Side Are You On}}?},
  shorttitle = {Bayesian {{Versus Orthodox Statistics}}},
  author = {Dienes, Zoltan},
  year = {2011},
  month = may,
  volume = {6},
  pages = {274--290},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691611406920},
  abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing\textemdash two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
  file = {/Users/shilaan/Zotero/storage/3A4FHA5X/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Ar.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {3}
}

@article{dienes2016,
  title = {How {{Bayes}} Factors Change Scientific Practice},
  author = {Dienes, Zoltan},
  year = {2016},
  month = jun,
  volume = {72},
  pages = {78--89},
  issn = {00222496},
  doi = {10.1016/j.jmp.2015.10.003},
  abstract = {Bayes factors provide a symmetrical measure of evidence for one model versus another (e.g. H1 versus H0) in order to relate theory to data. These properties help solve some (but not all) of the problems underlying the credibility crisis in psychology. The symmetry of the measure of evidence means that there can be evidence for H0 just as much as for H1; or the Bayes factor may indicate insufficient evidence either way. P-values cannot make this three-way distinction. Thus, Bayes factors indicate when the data count against a theory (and when they count for nothing); and thus they indicate when replications actually support H0 or H1 (in ways that power cannot). There is every reason to publish evidence supporting the null as going against it, because the evidence can be measured to be just as strong either way (thus the published record can be more balanced). Bayes factors can be B-hacked but they mitigate the problem because a) they allow evidence in either direction so people will be less tempted to hack in just one direction; b) as a measure of evidence they are insensitive to the stopping rule; c) families of tests cannot be arbitrarily defined; and d) falsely implying a contrast is planned rather than post hoc becomes irrelevant (though the value of pre-registration is not mitigated).},
  file = {/Users/shilaan/Zotero/storage/L563QCHY/Dienes - 2016 - How Bayes factors change scientific practice.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en}
}

@article{dodge1929,
  title = {A {{Method}} of {{Sampling Inspection}}},
  author = {Dodge, H. F. and Romig, H. G.},
  year = {1929},
  month = oct,
  volume = {8},
  pages = {613--631},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1929.tb01240.x},
  file = {/Users/shilaan/Zotero/storage/VHXRWEPD/Dodge and Romig - 1929 - A Method of Sampling Inspection.pdf},
  journal = {Bell System Technical Journal},
  language = {en},
  number = {4}
}

@article{fan2004,
  title = {Conditional {{Bias}} of {{Point Estimates Following}} a {{Group Sequential Test}}},
  author = {Fan, Xiaoyin and DeMets, David L. and Lan, K. K. Gordon},
  year = {2004},
  month = dec,
  volume = {14},
  pages = {505--530},
  issn = {1054-3406, 1520-5711},
  doi = {10.1081/BIP-120037195},
  file = {/Users/shilaan/Zotero/storage/UY8PJ5CJ/Fan et al. - 2004 - Conditional Bias of Point Estimates Following a Gr.pdf},
  journal = {Journal of Biopharmaceutical Statistics},
  language = {en},
  number = {2}
}

@article{fisher1922,
  title = {On the Mathematical Foundations of Theoretical Statistics},
  author = {Fisher, R. A.},
  year = {1922},
  month = jan,
  volume = {222},
  pages = {309--368},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.1922.0009},
  abstract = {Several reasons have contributed to the prolonged neglect into which the study of statistics, in its theoretical aspects, has fallen. In spite of the immense amount of fruitful labour which has been expended in its practical applications, the basic principles of this organ of science are still in a state of obscurity, and it cannot be denied that, during the recent rapid development of practical methods, fundamental problems have been ignored and fundamental paradoxes left unresolved. This anomalous state of statistical science is strikingly exemplified by a recent paper entitled "The Fundamental Problem of Practical Statistics," in which one of the most eminent of modern statisticians presents what purports to be a general proof of BAYES' postulate, a proof which, in the opinion of a second statistician of equal eminence, "seems to rest upon a very peculiar -- not to say hardly supposable -- relation."},
  file = {/Users/shilaan/Zotero/storage/XPUK5MSN/Fisher and Russell - 1922 - On the mathematical foundations of theoretical sta.pdf;/Users/shilaan/Zotero/storage/5VPZTTRH/rsta.1922.html},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  number = {594-604}
}

@book{fisher1956,
  title = {Statistical Methods and Scientific Inference},
  author = {Fisher, Ronald A.},
  year = {1956},
  pages = {viii, 175},
  publisher = {{Hafner Publishing Co.}},
  address = {{Oxford, England}},
  abstract = {An explicit statement of the logical nature of statistical reasoning that has been implicitly required in the development and use of statistical techniques in the making of uncertain inferences and in the design of experiments. Included is a consideration of the concept of mathematical probability; a comparison of fiducial and confidence intervals; a comparison of the logic of tests of significance with the acceptance decision approach; and a discussion of the principles of prediction and estimation. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  series = {Statistical Methods and Scientific Inference}
}

@article{fisher1999,
  title = {Carvedilol and the {{Food}} and {{Drug Administration}} Approval Process: An Introduction},
  shorttitle = {Carvedilol and the {{Food}} and {{Drug Administration}} Approval Process},
  author = {Fisher, L. D. and Moy{\'e}, L. A.},
  year = {1999},
  month = feb,
  volume = {20},
  pages = {1--15},
  issn = {0197-2456},
  doi = {10.1016/s0197-2456(98)00052-x},
  abstract = {We discuss briefly the new drug carvedilol (Coreg), a beta-blocker, alpha-blocker, and antioxidant. This drug was developed for congestive heart failure in a series of trials, four in the United States and one in Australia and New Zealand, briefly summarized in this document. We also summarize the classical paradigm of the U.S. Food and Drug Administration (FDA) for drug approval and the FDA's use of advisory committees. This document serves as background to the discussion of carvedilol's approval.},
  journal = {Controlled Clinical Trials},
  keywords = {Adrenergic alpha-Antagonists,Adrenergic beta-Antagonists,Carbazoles,Carvedilol,Drug Approval,Heart Failure,Humans,Propanolamines,Public Policy,Randomized Controlled Trials as Topic,United States,United States Food and Drug Administration},
  language = {eng},
  number = {1},
  pmid = {10027497}
}

@article{fisher1999a,
  title = {Carvedilol and the {{Food}} and {{Drug Administration}} ({{FDA}}) Approval Process: The {{FDA}} Paradigm and Reflections on Hypothesis Testing},
  shorttitle = {Carvedilol and the {{Food}} and {{Drug Administration}} ({{FDA}}) Approval Process},
  author = {Fisher, L. D.},
  year = {1999},
  month = feb,
  volume = {20},
  pages = {16--39},
  issn = {0197-2456},
  doi = {10.1016/s0197-2456(98)00054-3},
  abstract = {Carvedilol (Coreg), a beta- and alpha-blocker and an antioxidant drug, was evaluated for moderate to severe heart failure patients in a program containing four United States and one Australia/New Zealand study. The data were evaluated twice by the Cardiovascular and Renal Drugs Advisory Committee of the U.S. Food and Drug Administration (FDA). These meetings resulted in opposite decisions by the advisory committee. The crux of the argumentation was the two-positive-trial FDA paradigm. Carvedilol did not meet the usual paradigm because an exercise end point was not statistically different from placebo in three U.S. trials. Most other end points were highly significant, and death, which was monitored across the U.S. program, was different with p {$<$} 0.0001. Here we argue that the usual paradigm is very useful but not an absolute principle, that the usual paradigm can sometimes miss the strength of evidence even in the primary end points, and that rational decision making requires on occasion that other evidence must lead to approval. Control of the type I error rate should be taken very seriously, should rarely be violated, and serves the biomedical community well. It is not an absolute principle, however, but rather must be considered in context.},
  journal = {Controlled Clinical Trials},
  keywords = {Adrenergic alpha-Antagonists,Adrenergic beta-Antagonists,Carbazoles,Carvedilol,Drug Approval,Heart Failure,Humans,Propanolamines,Public Policy,Randomized Controlled Trials as Topic,United States,United States Food and Drug Administration},
  language = {eng},
  number = {1},
  pmid = {10027498}
}

@article{fleming1984,
  title = {Designs for Group Sequential Tests},
  author = {Fleming, Thomas R. and Harrington, David P. and O'Brien, Peter C.},
  year = {1984},
  month = dec,
  volume = {5},
  pages = {348--361},
  issn = {01972456},
  doi = {10.1016/S0197-2456(84)80014-8},
  abstract = {Several authors have proposed group sequential procedures to satisfy the ethical need in clinical trials for interim analyses. We propose here an alternative procedure that offers a good opportunity for early termination when initial results are extreme, while essentially maintaining the power provided by the procedure that applies when the corresponding test statistic is computed only at the predetermined time of final analysis. The new design is also sufficiently flexible to readily allow one to increase the maximum number of analyses for certain reasons such as unexpectedly slow accrual into the study, although not for reasons arising from consideration of outcomes observed at interim analysis.},
  file = {/Users/shilaan/Zotero/storage/8R2TFDBF/Fleming et al. - 1984 - Designs for group sequential tests.pdf},
  journal = {Controlled Clinical Trials},
  language = {en},
  number = {4}
}

@article{goodman1988,
  title = {Evidence and Scientific Research.},
  author = {Goodman, S N and Royall, R},
  year = {1988},
  month = dec,
  volume = {78},
  pages = {1568--1574},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.78.12.1568},
  file = {/Users/shilaan/Zotero/storage/VVNZAZCR/Goodman and Royall - 1988 - Evidence and scientific research..pdf},
  journal = {American Journal of Public Health},
  language = {en},
  number = {12}
}

@article{goodman1999,
  title = {Toward {{Evidence}}-{{Based Medical Statistics}}. 1: {{The P Value Fallacy}}},
  shorttitle = {Toward {{Evidence}}-{{Based Medical Statistics}}. 1},
  author = {Goodman, Steven N.},
  year = {1999},
  month = jun,
  volume = {130},
  pages = {995},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-130-12-199906150-00008},
  file = {/Users/shilaan/Zotero/storage/T37IXRVZ/Goodman - 1999 - Toward Evidence-Based Medical Statistics. 1 The P.pdf},
  journal = {Annals of Internal Medicine},
  language = {en},
  number = {12}
}

@article{goodman2007,
  title = {Stopping at {{Nothing}}? {{Some Dilemmas}} of {{Data Monitoring}} in {{Clinical Trials}}},
  shorttitle = {Stopping at {{Nothing}}?},
  author = {Goodman, Steven N.},
  year = {2007},
  month = jun,
  volume = {146},
  pages = {882},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-146-12-200706190-00010},
  file = {/Users/shilaan/Zotero/storage/H3JWNHJZ/Goodman - 2007 - Stopping at Nothing Some Dilemmas of Data Monitor.pdf},
  journal = {Annals of Internal Medicine},
  language = {en},
  number = {12}
}

@article{kruschke2018,
  title = {The {{Bayesian New Statistics}}: {{Hypothesis}} Testing, Estimation, Meta-Analysis, and Power Analysis from a {{Bayesian}} Perspective},
  shorttitle = {The {{Bayesian New Statistics}}},
  author = {Kruschke, John K. and Liddell, Torrin M.},
  year = {2018},
  month = feb,
  volume = {25},
  pages = {178--206},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1221-4},
  abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed ``the New Statistics'' (Cumming, 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
  file = {/Users/shilaan/Zotero/storage/L5N322QY/Kruschke and Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1}
}

@article{lakens2014,
  title = {Performing High-Powered Studies Efficiently with Sequential Analyses: {{Sequential}} Analyses},
  shorttitle = {Performing High-Powered Studies Efficiently with Sequential Analyses},
  author = {Lakens, Dani{\"e}l},
  year = {2014},
  month = dec,
  volume = {44},
  pages = {701--710},
  issn = {00462772},
  doi = {10.1002/ejsp.2023},
  abstract = {Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  file = {/Users/shilaan/Zotero/storage/5J354TXG/Lakens - 2014 - Performing high-powered studies efficiently with s.pdf},
  journal = {European Journal of Social Psychology},
  language = {en},
  number = {7}
}

@article{lakens2018,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  volume = {1},
  pages = {259--269},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245918770963},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  file = {/Users/shilaan/Zotero/storage/AWIPR9N4/Lakens et al. - 2018 - Equivalence Testing for Psychological Research A .pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {2}
}

@misc{lakens2020,
  title = {Statistical {{Inferences}}: {{Sequential Analysis}}},
  author = {Lakens, Dani{\"e}l},
  year = {2020},
  abstract = {Contribute to Lakens/statistical\_inferences development by creating an account on GitHub.},
  file = {/Users/shilaan/Zotero/storage/C949P845/14-sequential.html},
  howpublished = {https://github.com/Lakens/statistical\_inferences},
  journal = {GitHub},
  language = {en}
}

@techreport{lakens2020a,
  title = {The Practical Alternative to the P-Value Is the Correctly Used p-Value},
  author = {Lakens, Daniel},
  year = {2020},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/shm8v},
  abstract = {Due to the strong overreliance on p-values in the scientific literature some researchers have argued that p-values should be abandoned or banned, and that we need to move beyond p-values and embrace practical alternatives. When proposing alternatives to p-values statisticians often commit the `Statistician's Fallacy', where they declare which statistic researchers really `want to know'. Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p-value. As long as null-hypothesis tests have been criticized, researchers have suggested to include minimum-effect tests and equivalence tests in our statistical toolbox, and these tests (even though they return p-values) have the potential to greatly improve the questions researchers ask. It is clear there is room for improvement in how we teach p-values. If anyone really believes p-values are an important cause of problems in science, preventing the misinterpretation of p-values by developing better evidence-based education and user-centered statistical software should be a top priority. Telling researchers which statistic they should use has distracted us from examining more important questions, such as asking researchers what they want to know when they do scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  file = {/Users/shilaan/Zotero/storage/7UZQCVQ9/Lakens - 2019 - The practical alternative to the p-value is the co.pdf},
  keywords = {Meta-science}
}

@article{lan1983,
  title = {Discrete Sequential Boundaries for Clinical Trials},
  author = {Lan, K. K. Gordan and Demets, David L.},
  year = {1983},
  month = dec,
  volume = {70},
  pages = {659--663},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/70.3.659},
  abstract = {AbstractSUMMARY. Pocock (1977), O'Brien \&amp; Fleming (1979) and Slud \&amp; Wei (1982) have proposed different methods to construct discrete sequential boundari},
  file = {/Users/shilaan/Zotero/storage/YSXIDK5V/Gordon Lan and Demets - 1983 - Discrete sequential boundaries for clinical trials.pdf;/Users/shilaan/Zotero/storage/F39XWL3I/247777.html;/Users/shilaan/Zotero/storage/UY4QCVQ9/247777.html},
  journal = {Biometrika},
  language = {en},
  number = {3}
}

@misc{lawrencelivermorenationallaboratory2016,
  title = {All {{About}} That {{Bayes}}: {{Probability}}, {{Statistics}}, and the {{Quest}} to {{Quantify Uncertainty}}},
  shorttitle = {All {{About}} That {{Bayes}}},
  author = {{Lawrence Livermore National Laboratory}},
  year = {2016},
  month = sep,
  abstract = {Lawrence Livermore National Laboratory statistician Kristin Lennox delves into the history of statistics and probability in this talk, "All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty," given at LLNL on July 28, 2016.  Abstract: The great Bayesian vs. Frequentist war has raged within statistics for almost 100 years, much to the confusion of outsiders. The Bayesian/Frequentist question is no longer academic, with both styles of inference appearing frequently in scientific literature and even the news. In this talk, Kristin Lennox aims to explain the great divide to non-statisticians, and also to answer the most important statistical question of all: how does probability allow us to better understand our world? View the PowerPoint slides from the talk at http://www.slideshare.net/LivermoreLa...}
}

@article{mayo2016,
  title = {Don't {{Throw Out}} the {{Error Control Baby With}} the {{Bad Statistics Bathwater}}: {{A Commentary}}},
  author = {Mayo, Deborah G},
  year = {2016},
  volume = {70},
  pages = {1--2},
  file = {/Users/shilaan/Zotero/storage/M3VBJGV5/Mayo - Don’t Throw Out the Error Control Baby With the Ba.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {Online Discussion}
}

@book{mayo2018,
  title = {Statistical {{Inference}} as {{Severe Testing}}: {{How}} to {{Get Beyond}} the {{Statistics Wars}}},
  shorttitle = {Statistical {{Inference}} as {{Severe Testing}}},
  author = {Mayo, Deborah G.},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781107286184},
  abstract = {Mounting failures of replication in social and biological sciences give a new urgency to critically appraising proposed reforms. This book pulls back the cover on disagreements between experts charged with restoring integrity to science. It denies two pervasive views of the role of probability in inference: to assign degrees of belief, and to control error rates in a long run. If statistical consumers are unaware of assumptions behind rival evidence reforms, they can't scrutinize the consequences that affect them (in personalized medicine, psychology, etc.). The book sets sail with a simple tool: if little has been done to rule out flaws in inferring a claim, then it has not passed a severe test. Many methods advocated by data experts do not stand up to severe scrutiny and are in tension with successful strategies for blocking or accounting for cherry picking and selective reporting. Through a series of excursions and exhibits, the philosophy and history of inductive inference come alive. Philosophical tools are put to work to solve problems about science and pseudoscience, induction and falsification.},
  file = {/Users/shilaan/Zotero/storage/5KX7Z45D/D9DF409EF568090F3F60407FF2B973B2.html},
  isbn = {978-1-107-05413-4}
}

@article{mcgrath1981,
  title = {Dilemmatics: {{The Study}} of {{Research Choices}} and {{Dilemmas}}},
  shorttitle = {Dilemmatics},
  author = {McGrath, Joseph E.},
  year = {1981},
  month = nov,
  volume = {25},
  pages = {179--210},
  issn = {0002-7642, 1552-3381},
  doi = {10.1177/000276428102500205},
  file = {/Users/shilaan/Zotero/storage/H7BUE6ZP/McGrath - 1981 - Dilemmatics The Study of Research Choices and Dil.pdf},
  journal = {American Behavioral Scientist},
  language = {en},
  number = {2}
}

@article{miller2020,
  title = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing: {{The}} Independent Segments Procedure.},
  shorttitle = {A Simple, General, and Efficient Method for Sequential Hypothesis Testing},
  author = {Miller, Jeff and Ulrich, Rolf},
  year = {2020},
  month = oct,
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000350},
  abstract = {We propose a new sequential hypothesis testing procedure in which data are collected and analyzed in a series of independent segments. As in fixed-sample hypothesis testing and in previous sequential procedures, the overall \textvisiblespace{} level can be set to any desired value. Like other sequential procedures, the independent segments procedure generally requires smaller samples than fixed-sample procedures\textemdash{} often approximately 30\% smaller\textemdash to achieve the same \textvisiblespace{} level and statistical power. Relative to other sequential procedures, the new method has the advantages that it is simpler to use, requires fewer assumptions, and can be used with a wider array of statistical tests. Thus, in some circumstances the independent segments procedure may provide an attractive option for increasing the efficiency of statistical testing.},
  file = {/Users/shilaan/Zotero/storage/ABRC9QJY/Miller and Ulrich - 2020 - A simple, general, and efficient method for sequen.pdf},
  journal = {Psychological Methods},
  language = {en}
}

@article{morey2011,
  title = {Bayes Factor Approaches for Testing Interval Null Hypotheses.},
  author = {Morey, Richard D. and Rouder, Jeffrey N.},
  year = {2011},
  month = dec,
  volume = {16},
  pages = {406--419},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/a0024377},
  abstract = {Psychological theories are statements of constraint. The role of hypothesis testing in psychology is to test whether specific theoretical constraints hold in data. Bayesian statistics is well suited to the task of finding supporting evidence for constraint, because it allows for comparing evidence for 2 hypotheses against each another. One issue in hypothesis testing is that constraints may hold only approximately rather than exactly, and the reason for small deviations may be trivial or uninteresting. In the large-sample limit, these uninteresting, small deviations lead to the rejection of a useful constraint. In this article, we develop several Bayes factor 1-sample tests for the assessment of approximate equality and ordinal constraints. In these tests, the null hypothesis covers a small interval of non-0 but negligible effect sizes around 0. These Bayes factors are alternatives to previously developed Bayes factors, which do not allow for interval null hypotheses, and may especially prove useful to researchers who use statistical equivalence testing. To facilitate adoption of these Bayes factor tests, we provide easy-to-use software.},
  file = {/Users/shilaan/Zotero/storage/9T2ZNVKY/Morey and Rouder - 2011 - Bayes factor approaches for testing interval null .pdf},
  journal = {Psychological Methods},
  language = {en},
  number = {4}
}

@article{morey2016,
  title = {Why Most of Psychology Is Statistically Unfalsifiable ({{Version}} 1.0)},
  author = {Morey, Richard D and Lakens, Dani{\"e}l},
  year = {2016},
  doi = {10.5281/zenodo.838685},
  file = {/Users/shilaan/Zotero/storage/F255J25L/Morey and Lakens - Why most of psychology is statistically unfalsiﬁab.pdf},
  language = {en}
}

@article{nelson2018,
  title = {Psychology's {{Renaissance}}},
  author = {Nelson, Leif D. and Simmons, Joseph and Simonsohn, Uri},
  year = {2018},
  month = jan,
  volume = {69},
  pages = {511--534},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122216-011836},
  abstract = {In 2010\textendash 2012, a few largely coincidental events led experimental psychologists to realize that their approach to collecting, analyzing, and reporting data made it too easy to publish false-positive findings. This sparked a period of methodological reflection that we review here and call Psychology's Renaissance. We begin by describing how psychologists' concerns with publication bias shifted from worrying about file-drawered studies to worrying about p-hacked analyses. We then review the methodological changes that psychologists have proposed and, in some cases, embraced. In describing how the renaissance has unfolded, we attempt to describe different points of view fairly but not neutrally, so as to identify the most promising paths forward. In so doing, we champion disclosure and preregistration, express skepticism about most statistical solutions to publication bias, take positions on the analysis and interpretation of replication failures, and contend that meta-analytical thinking increases the prevalence of false positives. Our general thesis is that the scientific practices of experimental psychologists have improved dramatically.},
  file = {/Users/shilaan/Zotero/storage/AWYVDDGF/Nelson et al. - 2018 - Psychology's Renaissance.pdf},
  journal = {Annual Review of Psychology},
  language = {en},
  number = {1}
}

@article{neyman1933,
  title = {On the {{Problem}} of the {{Most Efficient Tests}} of {{Statistical Hypotheses}}},
  author = {Neyman, J. and Pearson, E. S.},
  year = {1933},
  volume = {231},
  pages = {289--337},
  publisher = {{The Royal Society}},
  issn = {0264-3952},
  journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character}
}

@article{pocock1977,
  title = {Group {{Sequential Methods}} in the {{Design}} and {{Analysis}} of {{Clinical Trials}}},
  author = {Pocock, Stuart J.},
  year = {1977},
  volume = {64},
  pages = {191--199},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335684},
  abstract = {In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes be statistically superior to standard sequential designs.},
  file = {/Users/shilaan/Zotero/storage/X53WL252/Pocock - 1977 - Group Sequential Methods in the Design and Analysi.pdf},
  journal = {Biometrika},
  number = {2}
}

@article{pocock1989,
  title = {Practical Problems in Interim Analyses, with Particular Regard to Estimation},
  author = {Pocock, Stuart J. and Hughes, Michael D.},
  year = {1989},
  month = dec,
  volume = {10},
  pages = {209--221},
  issn = {01972456},
  doi = {10.1016/0197-2456(89)90059-7},
  abstract = {This article considers some of the practical problems inherent in interim analyses and stopping rules for randomized clinical trials. Topics covered include group sequential designs, trials with unplanned interim analyses, estimation problems in clinical trials with planned interim analyses, and the balance between individual and collective ethics. Particular attention is paid to the fact that clinical trials that stop early are prone to exaggerate the magnitude of treatment effect. Accordingly, a Bayesian "shrinkage" method of analysis is proposed to help quantify the extent to which surprisingly large point and interval estimates of treatment difference in clinical trials that stop early should be moderated.},
  file = {/Users/shilaan/Zotero/storage/B3NHCU7D/Pocock and Hughes - 1989 - Practical problems in interim analyses, with parti.pdf},
  journal = {Controlled Clinical Trials},
  language = {en},
  number = {4}
}

@book{proschan2006,
  title = {Statistical Monitoring of Clinical Trials: A Unified Approach},
  shorttitle = {Statistical Monitoring of Clinical Trials},
  author = {Proschan, Michael A. and Lan, K. K. Gordan and Wittes, Janet Turk},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York, NY}},
  annotation = {OCLC: ocm71228590},
  file = {/Users/shilaan/Zotero/storage/JA23E4VM/Proschan et al. - 2006 - Statistical monitoring of clinical trials a unifi.pdf;/Users/shilaan/Zotero/storage/XTV5PIAT/Neyman Pearson On the problem of the most efficient tests of statistical hypotheses.pdf},
  isbn = {978-0-387-30059-7},
  keywords = {Bayes Theorem,Clinical trials,Clinical Trials,Data Interpretation; Statistical,Drugs,Statistical methods,Statistics,statistics \& numerical data,Testing},
  language = {en},
  lccn = {R853.C55 P76 2006},
  series = {Statistics for Biology and Health}
}

@article{rouder2016,
  title = {Is {{There}} a {{Free Lunch}} in {{Inference}}?},
  author = {Rouder, Jeffrey N. and Morey, Richard D. and Verhagen, Josine and Province, Jordan M. and Wagenmakers, Eric-Jan},
  year = {2016},
  month = jul,
  volume = {8},
  pages = {520--547},
  issn = {17568757},
  doi = {10.1111/tops.12214},
  abstract = {The field of psychology, including cognitive science, is vexed by a crisis of confidence. Although the causes and solutions are varied, we focus here on a common logical problem in inference. The default mode of inference is significance testing, which has a free lunch property where researchers need not make detailed assumptions about the alternative to test the null hypothesis. We present the argument that there is no free lunch; that is, valid testing requires that researchers test the null against a well-specified alternative. We show how this requirement follows from the basic tenets of conventional and Bayesian probability. Moreover, we show in both the conventional and Bayesian framework that not specifying the alternative may lead to rejections of the null hypothesis with scant evidence. We review both frequentist and Bayesian approaches to specifying alternatives, and we show how such specifications improve inference. The field of cognitive science will benefit because consideration of reasonable alternatives will undoubtedly sharpen the intellectual underpinnings of research.},
  file = {/Users/shilaan/Zotero/storage/NHDCCU9I/Rouder et al. - 2016 - Is There a Free Lunch in Inference.pdf},
  journal = {Topics in Cognitive Science},
  language = {en},
  number = {3}
}

@article{schnuerch2020,
  title = {Controlling Decision Errors with Minimal Costs: {{The}} Sequential Probability Ratio t Test.},
  shorttitle = {Controlling Decision Errors with Minimal Costs},
  author = {Schnuerch, Martin and Erdfelder, Edgar},
  year = {2020},
  month = apr,
  volume = {25},
  pages = {206--226},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000234},
  abstract = {For several years, the public debate in psychological science has been dominated by what is referred to as the reproducibility crisis. This crisis has, inter alia, drawn attention to the need for proper control of statistical decision errors in testing psychological hypotheses. However, conventional methods of error probability control often require fairly large samples. Sequential statistical tests provide an attractive alternative: They can be applied repeatedly during the sampling process and terminate whenever there is sufficient evidence in the data for one of the hypotheses of interest. Thus, sequential tests may substantially reduce the required sample size without compromising predefined error probabilities. Herein, we discuss the most efficient sequential design, the sequential probability ratio test (SPRT), and show how it is easily implemented for a 2-sample t test using standard statistical software. We demonstrate, by means of simulations, that the SPRT not only reliably controls error probabilities but also typically requires substantially smaller samples than standard t tests and other common sequential designs. Moreover, we investigate the robustness of the SPRT against violations of its assumptions. Finally, we illustrate the sequential t test by applying it to an empirical example and provide recommendations on how psychologists can employ it in their own research to benefit from its desirable properties.},
  file = {/Users/shilaan/Zotero/storage/R2N7UUBF/Schnuerch and Erdfelder - 2020 - Controlling decision errors with minimal costs Th.pdf},
  journal = {Psychological Methods},
  language = {en},
  number = {2}
}

@article{schonbrodt2017,
  title = {Sequential {{Hypothesis Testing With Bayes Factors}}: {{Efficiently Testing Mean Differences}}},
  author = {Sch{\"o}nbrodt, Felix D and Zehetleitner, Michael and Wagenmakers, Eric-Jan and Perugini, Marco},
  year = {2017},
  volume = {22},
  pages = {322--339},
  doi = {10.1037/met0000061},
  abstract = {Unplanned optional stopping rules have been criticized for inflating Type I error rates under the null hypothesis significance testing (NHST) paradigm. Despite these criticisms, this research practice is not uncommon, probably because it appeals to researcher's intuition to collect more data to push an indecisive result into a decisive region. In this contribution, we investigate the properties of a procedure for Bayesian hypothesis testing that allows optional stopping with unlimited multiple testing, even after each participant. In this procedure, which we call Sequential Bayes Factors (SBFs), Bayes factors are computed until an a priori defined level of evidence is reached. This allows flexible sampling plans and is not dependent upon correct effect size guesses in an a priori power analysis. We investigated the long-term rate of misleading evidence, the average expected sample sizes, and the biasedness of effect size estimates when an SBF design is applied to a test of mean differences between 2 groups. Compared with optimal NHST, the SBF design typically needs 50\% to 70\% smaller samples to reach a conclusion about the presence of an effect, while having the same or lower long-term rate of wrong inference.},
  file = {/Users/shilaan/Zotero/storage/GIUWUUV8/Schönbrodt et al. - Sequential Hypothesis Testing With Bayes Factors .pdf},
  journal = {Psychological Methods},
  language = {en},
  number = {2}
}

@article{schonbrodt2018,
  title = {Bayes Factor Design Analysis: {{Planning}} for Compelling Evidence},
  author = {Sch{\"o}nbrodt, Felix D and Wagenmakers, Eric-Jan},
  year = {2018},
  pages = {15},
  abstract = {A sizeable literature exists on the use of frequentist power analysis in the null-hypothesis significance testing (NHST) paradigm to facilitate the design of informative experiments. In contrast, there is almost no literature that discusses the design of experiments when Bayes factors (BFs) are used as a measure of evidence. Here we explore Bayes Factor Design Analysis (BFDA) as a useful tool to design studies for maximum efficiency and informativeness. We elaborate on three possible BF designs, (a) a fixed-n design, (b) an open-ended Sequential Bayes Factor (SBF) design, where researchers can test after each participant and can stop data collection whenever there is strong evidence for either H1 or H0, and (c) a modified SBF design that defines a maximal sample size where data collection is stopped regardless of the current state of evidence. We demonstrate how the properties of each design (i.e., expected strength of evidence, expected sample size, expected probability of misleading evidence, expected probability of weak evidence) can be evaluated using Monte Carlo simulations and equip researchers with the necessary information to compute their own Bayesian design analyses.},
  file = {/Users/shilaan/Zotero/storage/CR8XPKXL/Schönbrodt - 2018 - Bayes factor design analysis Planning for compell.pdf},
  journal = {Psychon Bull Rev},
  language = {en}
}

@article{simmons2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  file = {/Users/shilaan/Zotero/storage/ACEEKR6Y/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {11}
}

@techreport{stefan2020,
  title = {Efficiency in {{Sequential Testing}}: {{Comparing}} the {{Sequential Probability Ratio Test}} and the {{Sequential Bayes Factor Test}}},
  shorttitle = {Efficiency in {{Sequential Testing}}},
  author = {Stefan, Angelika and Sch{\"o}nbrodt, Felix D. and Evans, Nathan J. and Wagenmakers, Eric-Jan},
  year = {2020},
  month = nov,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/ry4fw},
  abstract = {In a sequential hypothesis test, the analyst checks at multiple steps during data collection whether sufficient evidence has accrued to make a decision about the tested hypotheses. As soon as sufficient information has been obtained, data collection is terminated. Here, we compare two sequential hypothesis testing procedures that have recently been proposed for use in psychological research: the Sequential Probability Ratio Test (SPRT; Schnuerch \& Erdfelder, 2020) and the Sequential Bayes Factor Test (SBFT; Sch\"onbrodt et al., 2017). We show that although the two methods have been presented as distinct methodologies in the past, they share many similarities and can even be regarded as two instances of the same overarching hypothesis testing framework. We demonstrate that the two methods use the same mechanisms for evidence monitoring and error control, and that differences in efficiency between the methods depend on the exact specification of the statistical models involved. Given the close relationship between the SPRT and SBFT, we argue that the choice of the sequential testing method should be regarded as a continuous choice within a unified framework rather than a dichotomous choice between two methods. We present several considerations researchers can make to navigate the design decisions in the SPRT and SBFT.},
  file = {/Users/shilaan/Zotero/storage/83H7HFWR/Stefan et al. - 2020 - Efficiency in Sequential Testing Comparing the Se.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{tiokhin2020,
  title = {Competition for Priority and the Cultural Evolution of Research Strategies},
  author = {Tiokhin, Leonid and Yan, Minhua and Morgan, Tom},
  year = {2020},
  month = feb,
  institution = {{MetaArXiv}},
  doi = {10.31222/osf.io/x4t7q},
  abstract = {Incentives for priority of discovery are hypothesized to harm scientific reliability. Here, we evaluate this hypothesis by developing an evolutionary agent-based model of a competitive scientific process. We find that rewarding priority of discovery causes populations to culturally evolve towards conducting research with smaller samples. This reduces research reliability and the information-value of the average study. Increased startup costs for setting up single studies and increased payoffs for secondary results (a.k.a. ``scoop protection'') attenuate the negative effects of competition. Further, large rewards for negative results promote the evolution of smaller sample sizes. Our results confirm the logical coherence of ``scoop protection'' reforms at several journals. Our results also imply that reforms to increase scientific efficiency, such as rapid journal turnaround times, may produce collateral damage by incentivizing lower-quality research; in contrast, reforms that increase startup costs, such as preregistration and registered reports, generate incentives for higher-quality research.},
  file = {/Users/shilaan/Zotero/storage/84FC4VKP/Tiokhin et al. - 2020 - Competition for priority and the cultural evolutio.pdf},
  language = {en},
  type = {Preprint}
}

@book{tippett1931,
  title = {The Methods of Statistics; an Introduction Mainly for Workers in the Biological Sciences,},
  author = {Tippett, L. H. C},
  year = {1931},
  publisher = {{Williams \& Norgate Ltd.}},
  address = {{London}},
  annotation = {OCLC: 2029473},
  language = {English}
}

@article{vandoorn2020,
  title = {The {{JASP}} Guidelines for Conducting and Reporting a {{Bayesian}} Analysis},
  author = {Van Doorn, Johnny and {van den Bergh}, Don and B{\"o}hm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Etz, Alexander and Evans, Nathan J. and Gronau, Quentin F. and Haaf, Julia M. and Hinne, Max and Kucharsk{\'y}, {\v S}imon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Gupta, Akash R. Komarlu Narendra and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G. and Wagenmakers, Eric-Jan},
  year = {2020},
  month = oct,
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01798-5},
  abstract = {Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed recommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared towards analyses performed with the open-source statistical software JASP, most guidelines extend to Bayesian inference in general.},
  file = {/Users/shilaan/Zotero/storage/GGCS46C4/van Doorn et al. - 2020 - The JASP guidelines for conducting and reporting a.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en}
}

@misc{wagenmakers,
  title = {A {{Compendium}} of {{Clean Graphs}} in {{R}}},
  author = {Wagenmakers, Eric-Jan and Gronau, Quentin F.},
  file = {/Users/shilaan/Zotero/storage/PLHAZAIR/index.html},
  howpublished = {https://www.shinyapps.org/apps/RGraphCompendium/index.php\#prior-and-posterior}
}

@article{wagenmakers2007,
  title = {A Practical Solution to the Pervasive Problems Ofp Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  volume = {14},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  file = {/Users/shilaan/Zotero/storage/54BTUF58/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {5}
}

@article{wagenmakers2018,
  title = {Bayesian Inference for Psychology. {{Part I}}: {{Theoretical}} Advantages and Practical Ramifications},
  shorttitle = {Bayesian Inference for Psychology. {{Part I}}},
  author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and {\v S}m{\'i}ra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
  year = {2018},
  month = feb,
  volume = {25},
  pages = {35--57},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1343-3},
  abstract = {Bayesian parameter estimation and Bayesian hypothesis testing present attractive alternatives to classical inference using confidence intervals and p values. In part I of this series we outline ten prominent advantages of the Bayesian approach. Many of these advantages translate to concrete opportunities for pragmatic researchers. For instance, Bayesian hypothesis testing allows researchers to quantify evidence and monitor its progression as data come in, without needing to know the intention with which the data were collected. We end by countering several objections to Bayesian hypothesis testing. Part II of this series discusses JASP, a free and open source software program that makes it easy to conduct Bayesian estimation and testing for a range of popular statistical scenarios (Wagenmakers et al., this issue).},
  file = {/Users/shilaan/Zotero/storage/CVJH39RW/Wagenmakers et al. - 2018 - Bayesian inference for psychology. Part I Theoret.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1}
}

@book{wald1973,
  title = {Sequential Analysis},
  author = {Wald, Abraham},
  year = {1973},
  publisher = {{Dover Publications}},
  address = {{New York}},
  file = {/Users/shilaan/Zotero/storage/HY49WK9B/Wald - 1973 - Sequential analysis.pdf},
  isbn = {978-0-486-61579-0},
  keywords = {Sequential analysis},
  language = {en},
  lccn = {QA279.7 .W34 1973}
}

@book{wassmer2016,
  title = {Group {{Sequential}} and {{Confirmatory Adaptive Designs}} in {{Clinical Trials}}},
  author = {Wassmer, Gernot and Brannath, Werner},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-32562-0},
  file = {/Users/shilaan/Zotero/storage/B7BMC7DS/Wassmer and Brannath - 2016 - Group Sequential and Confirmatory Adaptive Designs.pdf},
  isbn = {978-3-319-32560-6 978-3-319-32562-0},
  language = {en},
  series = {Springer {{Series}} in {{Pharmaceutical Statistics}}}
}

@article{westberg1985,
  title = {Combining {{Independent Statistical Tests}}},
  author = {Westberg, Margareta},
  year = {1985},
  volume = {34},
  pages = {287--296},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2987655},
  abstract = {In the present study two well-known combination methods, Fisher's and Tippett's, are compared according to their power. The calculations are made for normally and chi-square distributed test statistics. None of the two procedures is uniformly better than the other according to the power but sometimes the power curves cross each other. The calculated power-graphs give guidelines for when to use Fisher's method and when to use Tippett's.},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  number = {3}
}


